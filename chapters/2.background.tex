\chapter{Background}
In this chapter we review and introduce some essential concepts used in this work. First, we give a short introduction on the Haskell Programming Language. Later we describe briefly the fundamentals of concurrency and the approaches for concurrent programming in Haskell. Finally, we discuss important concepts of software energy consumption.

\section{Haskell}
Haskell is a \emph{purely functional} programming language. By functional, we mean that functions are the building blocks of programs written in Haskell. By pure, we mean that no side-effect happens when evaluating a function. While in imperative programming languages, a program is expressed as a sequence of instructions that mutates data, a program in functional programming languages is expressed as a composition of expressions where all state is controlled by passing arguments to function calls and returning values from them. This property is called \emph{referential transparency}. It guarantees that in a given execution context, a function executed with a given argument will always produce the same result. This property makes it easier to reason about programming as it enforces that a program's behavior cannot depend on history.

Haskell is also a \emph{lazy} programming language. Lazy refers to a non-strict evaluation strategy also known as \emph{call-by-need}. This strategy delays the evaluation of an expression until its value is needed. It avoids repeated evaluations which can lead to performance improvements. This strategy also makes it possible to construct potentially infinite data structures.

\emph{Recursion} is the norm in a purely functional programming language as regular iterative loops require state mutation. To make it easier to express recursive functions, Haskell also has \emph{pattern matching}. In Code \ref{code:fact}, we can see an example of a recursive function using pattern matching. Line 1 is the base case, when the \texttt{factorial} receives zero as parameter, and Line 2 is the general case.

\begin{listing}
  \begin{minted}{haskell}
factorial :: Int -> Int
factorial 0 = 1
factorial x = x * factorial (x - 1)
  \end{minted}
  \caption{A recursive factorial function}
  \label{code:fact}
\end{listing}

Another characteristic of Haskell is that functions are values. It means that a function can receive other functions as argument, and also can be the result of a function evaluation. This feature is known as \emph{high-order functions}. It enables very popular functional patterns such as \texttt{map}, \texttt{filter} and \texttt{reduce}.

Functions in Haskell can also be \emph{polymorphic}, which means that a function can be generalized to work with multiple types instead of a single one. Other programming languages have similar features such as \emph{generics} in Java and \emph{templates} in C++. Code \ref{code:poly} shows an example of a polymorphic function that can reverse a list of any type. In this example, \texttt{t} is a \emph{parametric type}~\citep{cardelli:1985} used to bind the input type to the output type of \texttt{reverse}.

\begin{listing}
  \begin{minted}{haskell}
reverse :: [t] -> [t]
reverse l = rev l []
  where
    rev []     a = a
    rev (x:xs) a = rev xs (x:a)
  \end{minted}
  \caption{A polymorphic function to reverse a list}
  \label{code:poly}
\end{listing}

In Haskell, a developer can also extend the built-in primitive types by defining new \emph{abstract data types}. Code \ref{code:tree} shows an example defining the \texttt{Tree} data type and the function \texttt{depth} to calculate the depth of the tree. As this example shows, the parametric polymorphism also works for abstract data types. Here, \texttt{Tree} have two constructors \texttt{Node} and \texttt{Empty}, where the first one holds three elements: the value of the node, the, left and right sub-trees. Pattern matching can be used to walk through a \texttt{Tree} as shown in Lines 5 and 6.

\begin{listing}
  \begin{minted}{haskell}
data Tree t = Node t (Tree t) (Tree t)
            | Empty

depth :: Tree t -> Int
depth Empty        = 0
depth (Node _ l r) = 1 + max (depth l) (depth r)
  \end{minted}
  \caption{A data type for binary tree and a function to calculate its depth}
  \label{code:tree}
\end{listing}

There is also a concept called \emph{typeclasses} that enhances the definition of new types in Haskell. A typeclass is similar to an interface. It defines a set of functions that can be applied to a particular type. Typeclasses were designed as a way for implementing ad hoc polymorphism in Haskell. Code \ref{code:tree-eq} shows an example of instantiation of the \texttt{Eq} typeclass for the \texttt{Tree} type we defined earlier. It states that a \texttt{Tree} is comparable for equality if its contained type is also comparable for equality.

\begin{listing}
  \begin{minted}{haskell}
instance Eq a => Eq (Tree a) where
  (==) Empty Empty = True
  (==) Empty (Node _ _ _) = False
  (==) (Node _ _ _) Empty = False
  (==) (Node x xl xr) (Node y yl yr) = (x == y) && (xl == yl) && (xr == yr)
  \end{minted}
  \caption{Definition of an Eq typeclass instance for the Tree data type}
  \label{code:tree-eq}
\end{listing}

The \texttt{Monad} typeclass is particularly important for Haskell because it allow developers to emulate mutable behavior and side-effects in a purely functional manner. Its definition can be seen in Code \ref{code:monad}. The most important functions of this interface are \texttt{(>==)}, also known as \emph{bind}, and \texttt{return}, also called \emph{unit}. The first one binds the contained value of the monad \texttt{m} to the argument of its argument function, and the second wraps a value in the monad \texttt{m} and returns it. It is a common idiom to call a \emph{monad} any type that is an instance of class \texttt{Monad}.

\begin{listing}
  \begin{minted}{haskell}
class Monad m where
  (>>=)  :: m a -> (a -> m b) -> m b
  (>>)   :: m a -> m b -> m b
  return :: a -> m a
  fail   :: String -> m a
  \end{minted}
  \caption{Definition of type class Monad}
  \label{code:monad}
\end{listing}

Two monads are particularly important for this work: \texttt{IO} and \texttt{STM}. The first one defines an environment to execute input/output operations. The second defines an environment for Software Transactional Memory, which is presented in Section \ref{sec:haskell-conc}. In Code \ref{code:io}, we have an example of the use of \texttt{Monad} operators to perform I/O. For instance, the functions \texttt{putStrLn} and \texttt{getLine} from the module \texttt{System.IO} prints and reads a \texttt{String} from the standard I/O, respectively. The result type of \texttt{main} is \texttt{IO ()}, where \texttt{()} is an empty tuple value. It represents that no value is returned, analogous to the type \texttt{void} on imperative languages.

\begin{listing}
  \begin{minted}{haskell}
main :: IO ()
main = putStrLn "What is your name?" >> getLine
       >>= \name -> putStrLn ("Hey " ++ name ++ ", you rock!")
  \end{minted}
  \caption{An example using the Monad operators for the IO type}
  \label{code:io}
\end{listing}

\begin{listing}
  \begin{minted}{haskell}
main :: IO ()
main = do
  putStrLn "What is your name?"
  name <- getLine
  putStrLn ("Hey " ++ name ++ ", you rock!")
  \end{minted}
  \caption{An example using the do notation}
  \label{code:io-do}
\end{listing}

Finally, there is a notation in Haskell that makes it easier to express operation within a monad, the \emph{do-notation}. Code \ref{code:io-do} shows the same example of Code \ref{code:io} rewritten using do-notation. Using \texttt{do}, a series of monadic function calls is sequenced as if in an imperative program. It works mainly as syntactic sugar for \texttt{(>==)} and \texttt{(>>)} calls, binding variables that later become argument of other functions and mainly sequentially composing the calls.


\section{Concurrency}
Concurrency and concurrent programming are on the rise nowadays due to the proliferation of multicore processors. However, these concepts are present in computer science since the early 1970s with the introduction of \emph{time-sharing}~\citep{lea:2006}. This model enables, for example, multi-tasking, which allows users to do several things at the same time in a computer such as browsing the internet, playing music and writing a document. To make this possible, the operating system scheduler has to share the processor time between all other processes that want to use this resource. By doing so, the user feels like the programs are executing at the same time although the processor is actually executing each program in a different time slice.

Concurrency is also usually associated (sometimes indistinctly) to parallelism. Although they are similar concepts, they are not the same. Concurrency consists in logically structuring programs in distinct control flows. The execution of these control flows is interlaced to simulate simultaneity when the underlying processor is single core. On the other hand, parallelism is concerned with improving a program's performance by executing several computations in parallel, which require a multicore processor. Depending on the number of cores available, the execution of a program can be literally parallel, entirely time-shared, or a combination of both.

In operating systems, there are two well-known concurrent programming abstractions to express an alternative flow of control: \emph{processes} and \emph{threads}. A process is a self-contained execution environment that holds all the information needed to run a program. Creating a new process is an onerous operation due to the significant number of resources it requires such as memory, registers and address space. For this reason, they are known to be \emph{heavyweight}. On the other hand, threads are the smallest concurrency unit in modern operating systems~\citep{tanenbaum:2007}. As threads are contained within a process, there is a low overhead associated with creating new threads because they all share the same memory and address space. Due to this fact, threads are known to be \emph{lightweight}. For instance, creating a thread is around 100 times faster than creating a process in POSIX systems~\citep{butenhof:1997}.

In high-level programming languages, threads are the weapon of choice for concurrent programming. Mainly because, when compared to processes, they are faster and easier to manage. But creating new threads by itself is not enough for building complex concurrent systems. We need mechanisms to enable coordination among the different flows of control. There are two mainstream communication strategies that enables cooperation between threads. The first one is sharing memory. In this approach, threads communicate with each other through reads and writes to a common memory location. To ensure program consistency, they need to control the access to these common locations. This control is commonly achieved through different synchronization mechanism such as semaphores and mutexes. In these strategies, a thread has to acquire a lock as a way to communicate its access to a resource. However, this method is very prone to concurrency hazards such as deadlocks and livelocks~\citep{herlihy:2012}.

The other communication mechanism is message passing. In this approach, the components communicate by exchanging messages. Each component (or process) is isolated, which means that there is no memory sharing. Two particular styles of message passing are popular: the Actor Model~\citep{agha:1986} and \ac{csp}~\citep{hoare:1978}. The main difference is that the first is \emph{asynchronous} while the second is \emph{synchronous}. In \acs{csp}, a process that is sending a message blocks until the receiver accepts it. In the Actor Model, the messages are kept in the receiver's mailbox to avoid blocking the sender. The Actor Model is very popular in functional programming languages such as Erlang~\citep{armstrong:2007}, Scala~\citep{haller:2009} and Haskell~\citep{epstein:2011}. The \acs{csp} model serves as inspiration for the concurrency abstractions of the Go programming language~\citep{pike:2012}.

Another mechanism to consistently coordinate concurrent threads in a shared memory scenario is \ac{tm}. The basic idea is very simple. The runtime should take care of controlling the access to common memory locations. It uses an abstraction called \emph{transaction} that behaves similarly to database transactions. In this approach, code that access shared memory is wrapped inside a transaction. Any conflicts that occur when threads are concurrently accessing the same location activate recovery strategies. These strategies ensure that each transaction is executed as if atomically and in isolation (with no intermediary state visible to other threads).

The first time this idea of using an abstraction such as database transactions to ensure consistency of shared data was presented by \cite{lomet:1977}. It was then formalized as \acl{tm} by \cite{herlihy:1993}. They proposed a hardware-supported transactional memory as a mechanism for building lock-free data structures. Although the original proposal required specialized hardware, we can now see implementations of \ac{stm}~\citep{shavit:1995}. There are several distinct implementations for different programming languages, including C/C++, Clojure, Java, Scala, and Haskell. Both Clojure and Haskell have  \acs{stm} support built into the core language. We will provide more details on Haskell's \acs{stm} in the next section.

It is important to note that there is a fundamental difference between the two approaches we saw for consistent data sharing. While locking strategies such as semaphores and mutexes tries to avoid conflicts by not allowing concurrent access to shared data. \acs{stm} assumes that no conflict will happen and, in case it happens, something is done to recover. For this reason, the previous is called \emph{pessimistic concurrency} while the later is called \emph{optimistic concurrency}.


\section{Concurrency in Haskell}\label{sec:haskell-conc}
The main component of the Haskell ecosystem is the \ac{ghc}~\citep{smpj:1993}. \acs{ghc} provides a complete infrastructure for building, running, debugging and profiling Haskell programs. It is composed of two core pieces: the compiler itself and the runtime system. The compiler translates source language into assembly code executable by a native host. The runtime system is a support library for primitive language services such as memory management and IO. To enable concurrency in Haskell, we have a combination of both high-level constructs on the source language and a low-level infrastructure that is part of the \ac{rts}~\citep{li:2007}.

\emph{Haskell threads} (also known as \emph{green threads}) are the main abstraction for concurrent programming in Haskell. These are special threads managed by the \acl{rts}. They are multiplexed over a much smaller number of operating system threads. The \ac{rts} takes care of scheduling green threads to execute on a set of \emph{virtual processors}. These virtual processors are also known as \acp{hec} or \emph{capabilities}. Each one can run one Haskell thread at a time. Each capability also has a run queue for keeping the Haskell threads that will run next.

% - a capability is animated by one or more operating system threads.
% --- say that each HEC can have more than one OS thread?
% - number of capabilities is equal to the number of Haskell threads that can run physically in parallel
The runtime system has an internal scheduler to manage the green threads. It uses a round-robin scheduling policy to manage the capabilities' run queue. So, each thread in the queue runs for a time slice\footnote{The default rescheduling time is 20ms. The developer can change it by passing a different value to the \ac{rts} via the \texttt{-i} command line argument} before being interrupted to the next one to run. The scheduler also performs load-balancing of Haskell threads. It moves threads from a capability's run queue to another to avoid CPU idle time\footnote{The strategy used to move work from one capability to another is currently fixed, but there are some work towards making this policy customizable~\citep{siva:2014}}. These features make Haskell threads considerably more lightweight than regular OS threads. The GHC documentation states that: \emph{"Typically Haskell threads are an order of magnitude or two more efficient (in terms of both time and space) than operating system threads."}\footnote{\url{http://hackage.haskell.org/package/base-4.8.2.0/docs/Control-Concurrent.html\#g:11}} Another advantage of Haskell threads is the low context-switching overhead when comparing to OS threads. This is crutial for some systems with a high performance requirement such as web servers~\citep{voellmy:2013}.

\begin{figure}[htp]
  \centering
  \caption{Layers of concurrency in a Haskell stack}
  \includegraphics[width=\columnwidth]{images/haskell-concurrency-layers}
  \footnotesize{Source: Made by the author. Inspired on slides from "GHC illustrated" presentation by Takenobu T.}
  \label{fig:haskell-conc-layers}
\end{figure}

In \figref{fig:haskell-conc-layers}, we show the various layers involved in a concurrent system written in Haskell. In this example, the underlying machine has two cores. For this reason, there are two \acp{hec} associated with two OS threads. Although depicted this way, a developer can configure it differently. For instance, the number of capabilities can be set when the program is executed  by passing a command line argument for the \ac{rts} (in this case, \texttt{-N}). Also, it is important to note that a program in Haskell can be linked with either threaded \ac{rts} or non-threaded \ac{rts}. The program should be linked to the threaded runtime to leverage multicore processors (as in \figref{fig:haskell-conc-layers}).\footnote{This can be achieved by passing \texttt{-threaded} to \ac{ghc} when compiling the program}

\begin{listing}
  \caption{Type signatures for thread creation functions}
  \begin{minted}{haskell}
forkIO :: IO () -> IO ThreadId
forkOn :: Int -> IO () -> IO ThreadId
forkOS :: IO () -> IO ThreadId
  \end{minted}
  \label{code:fork-sig}
\end{listing}

From a more higher-level perspective, the original framework for concurrency in Haskell is called \emph{Concurrent Haskell}~\citep{smpj:1996}. It represents a thread as a computation in the IO monad. We have three main functions to create a new thread in Haskell as \autoref{code:fork-sig} shows. \forkIO spawns a Haskell thread. It takes an IO computation to be executed concurrently and returns a pointer to the newly created thread. \forkOn also spawns a Haskell thread but lets the developer specify on which capability the thread should run. Unlike threads created with \forkIO, the scheduler cannot migrate threads created with \forkOn from one capability to another. For instance, if a program creates all its threads using \forkOn, the scheduler will not be able to perform load-balancing. Finally, \forkOS spawns a \emph{bound thread}. A bound thread is a Haskell thread that is bound to an specific OS thread. They are treated by the scheduler the same way as other Haskell threads. The only difference is when it is time to run a bound thread. The capability has to run it on its bound OS thread. \figref{fig:haskell-threads} shows the difference between each thread creation function.

\begin{figure}[htp]
  \centering
  \caption{Thread creation functions}
  \includegraphics[width=\columnwidth]{images/haskell-threads-placeholder}
  \footnotesize{Source: Made by the author. Inspired on slides from "GHC illustrated" presentation by Takenobu T.}
  \label{fig:haskell-threads}
\end{figure}

The basic concurrency control primitive of Haskell is the \MVar. A value of type \MVar~\texttt{t} is a mutable location that is either empty or contains a value of type \texttt{t}. \autoref{code:mvar-sig} shows the \acs{api} for manipulating an \MVar. The more commonly used functions are \newMVar, \newEmptyMVar, \takeMVar and \putMVar. The first one creates an \MVar with a value inside. The second creates an empty \MVar. The \takeMVar function takes a value from an \MVar, returning it in the IO monad. The operation returns immediatly if the \MVar is full. For empty ones it will block until they are filled. The opposite applies to \putMVar. The function puts a value in an \MVar. It will return immediatly if the \MVar is empty. Otherwise, the function will block until the \MVar is emptied. An \MVar combine both locking and condition-based synchronization in a single primitive. It also has a formally defined semantics~\citep{smpj:1996}.

\begin{listing}
  \caption{The \MVar interface}
  \begin{minted}{haskell}
-- Type definition
data MVar a

-- MVar manipulation
takeMVar     :: MVar a -> IO a
putMVar      :: MVar a -> a -> IO ()
tryTakeMVar  :: MVar a -> IO (Maybe a)
tryPutMVar   :: MVar a -> a -> IO Bool
isEmptyMVar  :: MVar a -> IO Bool
swapMVar     :: MVar a -> a -> IO a

-- MVar creation
newMVar      :: a -> IO (MVar a)
newEmptyMVar :: IO (MVar a)
  \end{minted}
  \label{code:mvar-sig}
\end{listing}

In Haskell, we can also use software transactional memory to coordinate concurrency. Its implementation is called \acs{stm} Haskell~\citep{harris:2005}. \autoref{code:stm-api} shows the API for \acs{stm} in Haskell.
% [[ TVar + STM monad ]]

\begin{listing}
  \caption{The \STM interface}
  \begin{minted}{haskell}
-- The STM monad
data STM a
instance Monad STM -- support "do" notation and sequencing

-- Transactional variable
data TVar a
newTVar         :: a -> STM (TVar a)
readTVar        :: TVar a -> STM a
writeTVar       :: TVar a -> a -> STM ()

-- Transactional MVar
data TMVar a
newTMVar        :: a -> STM (TMVar a)
takeTMVar       :: TMVar a -> STM a
putTMVar        :: TMVar a -> a -> STM ()
tryTakeTMVar    :: TMVar a -> STM (Maybe a)
tryPutTMVar     :: TMVar a -> a -> STM Bool

-- Running STM computations
atomically      :: STM a -> IO a
retry           :: STM a
orElse          :: STM a -> STM a -> STM a
  \end{minted}
  \label{code:stm-api}
\end{listing}

% [[ TMVar ]]

% [[ Composability + retry + orElse]]


\section{Software Energy Consumption}
\lipsum[1-4]
