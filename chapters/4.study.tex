\chapter{The Energy Efficiency of Concurrent Haskell}\label{chp:study}
In this chapter, we present an empirical study evaluating the performance and energy consumption characteristics of three thread management constructs and three data-sharing primitives of Concurrent Haskell. We start by providing a brief overview (\secref{sec:overview}) of the study and state the research questions. \secref{sec:setup} describes the benchmark we developed, the environment and the methodology we used. Sections \ref{sec:results}, \ref{sec:fasta}, and \ref{sec:discussion} present our results. Finally, \secref{sec:threats} list the threats to validity.

\section{Overview}\label{sec:overview}
Recently, the software engineering community has been showing a keen interest in researching about the development of energy-efficient software. As multicore architectures are the norm nowadays, this interest also extends for energy-efficiency from the perspective of concurrent software running on multicore architectures~\cite{trefethen:2013,ribic:2014, pinto:2014}. However, the same cannot be said about functional programming. This is unfortunate as functional programming languages are seen as a good alternative for making concurrent programming less error-prone. % [REF?]
Particularly, Haskell provides a solid foundation for building concurrent software, and we know very little about its energy behavior.

We believe that the first step towards optimizing energy consumption of concurrent programs is to gain a comprehensive understanding of their energy behaviors. This chapter presents an empirical study that aims to understand the energy behaviors of Haskell concurrent programs on multicore architectures. In particular, our study focuses on how programmer's decisions may impact energy consumption and performance. The following questions motivate our research:

\begin{enumerate}[label=\RQ{\arabic*}.]
  \item Do alternative \textit{thread management constructs} have different impacts on energy consumption?
  \item Do alternative \textit{data-sharing primitives} have different impacts on energy consumption?
  \item What is the relationship between the \textit{number of capabilities} and energy consumption?
\end{enumerate}

To answer \RQ{1}, we select three thread management constructs: \forkIO, \forkOn and \forkOS. As we saw in \secref{sec:haskell-conc}, these are the three functions available in Haskell to spawn a new thread of execution. To answer \RQ{2}, we select three data-sharing primitives: \MVar, \TMVar and \TVar. Given these constructs, our investigation aims to understand how the energy consumption can be impacted by changing the concurrency primitives used in a Haskell program. Finally, to answer \RQ{3}, we explore various capabilities settings to see how it can impact energy consumption and performance on a multicore environment.


\section{Study Setup}\label{sec:setup}
In this section we describe the benchmarks that we analyzed, the infrastructure and the methodology that we used to perform the experiments.

\subsection{Benchmarks}
We selected a variety of concurrent Haskell programs to use as benchmarks in our study, listed as follows. Benchmarks \ref{bench:chameneos}-\ref{bench:spectral} are from \ac{clbg}\footnote{http://benchmarksgame.alioth.debian.org/}. \acs{clbg} is a benchmark suite aiming to compare the performance of various programming languages. Benchmark \ref{bench:dining} is from  Rosetta Code \footnote{http://rosettacode.org/}, a code repository of solutions to common programming tasks. Benchmarks \ref{bench:tsearch}-\ref{bench:warp} were developed by us.

\begin{enumerate}
  \item \label{bench:chameneos}\chameneos: In this benchmark chameneos creatures go to a meeting place and exchange colors with a meeting partner. It encodes symmetrical cooperation between threads.

  \item \fasta:  This benchmark generates random DNA sequences and writes them in FASTA format\footnote{https://en.wikipedia.org/wiki/FASTA\_format}. The size of the generated DNA sequences is in the order of hundreds of megabytes. In this benchmark,  each worker synchronizes with the previous one to output the sub-sequences of DNA in the correct order.

  \item \knucleotide: This benchmark takes a DNA sequence and counts the occurrences and the frequency of nucleotide patterns. This benchmark employs string manipulation and hashtable updates intensively. There is no synchronization in the program besides the main thread waiting for the result of each worker.

  \item \mandelbrot: A mandelbrot is a mathematical set of points whose boundary is a distinctive and easily recognizable two-dimensional fractal shape. Mandelbrot set images are created by sampling complex numbers and determining for each one whether the result tends toward infinity when a particular mathematical operation is iterated on it. The only synchronization point in this benchmark is the main thread waiting for the result of each worker.

  \item \regex: This benchmark implements a string-based algorithm that performs multiple regular expression operations, match and replace, over a DNA sequence. The only synchronization point is the main thread waiting for the result of each worker.

  \item \label{bench:spectral}\spectral: The spectral norm is the maximum singular value of a matrix. It synchronizes the workers using a cyclic barrier.

  \item \label{bench:dining}\dining: An implementation of the classical concurrent programming problem posed by~\citeonline{dijkstra:1971}. The philosophers perform no work besides manipulating the forks and printing a message when eating.

  \item \label{bench:tsearch}\tsearch: A parallel text search engine. This benchmark searches for occurrences of a sentence in all text files in a directory and its sub-directories. It is based on a previous empirical study comparing STM and locks~\cite{pankratius:2011}.

  \item \label{bench:warp}\warp: Runs a set of queries against a Warp server retrieving the resulting webpages. Warp is the default web server used by the \ac{wai}, part of the Yesod Web Framework. This benchmark was inspired by the Tomcat benchmark from DaCapo~\cite{blackburn:2006}.
\end{enumerate}

We selected the benchmarks based on their diversity. For instance, \chameneos and \dining are synchronization-intensive programs. \mandelbrot and \spectral are CPU-intensive and scale well on a multicore machine. \knucleotide and \regex are CPU- and memory-intensive, while \warp is IO-intensive. \tsearch combines IO and CPU operations, though much of the work it performs is CPU-intensive. \fasta is peculiar in that is CPU-, memory-, synchronization- and IO-intensive.

Also, some benchmarks have a fixed number of workers (\chameneos, \knucleotide, \regex, and \dining) and others spawn as many workers as the number of capabilities (\fasta, \mandelbrot, \spectral, \tsearch and \warp). For the \dining benchmark, it is possible to establish prior to execution the number of workers.

\subsection{Experimental Environment}
For our study, all experiments were conducted on a machine with 2x10-core Intel Xeon E5-2660 v2 processors (Ivy Bridge microarchitecture), 2.20 GHz, with 256GB of DDR3 1600MHz memory. It has three cache levels (L1, L2, L3): L1 with 32KB per core, L2 with 256KB per core, and L3 with 25MB per socket (Smart cache). This machine runs the Ubuntu Server 14.04.3 LTS (kernel 3.19.0-25) operating system, \acs{ghc} 7.10.2, and our modified Criterion based on version 1.1.0 of the official release (\secref{sec:criterion}).

All experiments were performed with no other load on the OS. We conform to the default settings of the operating system. For each benchmark, we used the same compilation and runtime flags employed in the original benchmark suites. This is important to preserve the same performance characteristics as intended by the implementers. Both the energy consumption and the performance are measured by Criterion.

\subsection{Methodology}
Most of the benchmarks we used were implemented in their respective suites using a single thread management construct and a single data-sharing primitive. For example, the ones from \acs{clbg} use originally \forkIO and \MVar. In order to analyze the impact of both thread management constructs and data-sharing primitives, we manually refactored each benchmark to create new \emph{variants} using different constructs. Changing the thread management construct is a straightforward process. The functions have almost the same signature. The only exception is \forkOn that requires an extra argument representing in which capability the thread will run. For this study, we distributed the threads evenly among the capabilities when using \forkOn. However, changing the data-sharing primitive is not always simple since the semantics of {\TVar}s and {\MVar}s differ considerably. To properly perform this refactoring, we used the techniques proposed by~\citeonline{soares-neto:2014} to refactor the benchmarks to use STM.

As a result of this process, each benchmark has up to nine distinct variants covering a number of different combinations of both thread management constructs and data-sharing primitives. However, it is important to note that there are some cases like \dining where not all possible combinations were created. In this particular implementation, the shared variable is also used as a condition-based synchronization mechanism. In such cases, where {\MVar}s are used to implement both mutual exclusion and condition-based synchronization, we did not create the \TVar variants. We limited ourselves to the variants using {\MVar}s and {\TMVar}s since a \TMVar is very similar to an \MVar while executing inside a transaction. In other benchmarks like \tsearch and \warp, we changed only the thread management construct as they are complex applications and it would not be straightforward to change the synchronization primitives without introducing potential bugs.

In the end, each variant we created is represented by a standalone executable. This executable is a Criterion microbenchmark that performs the experiment by calling the original program entry point multiple times. We run each benchmark under nine different configurations of capabilities. We used the following values for \texttt{N}: $\{1, 2, 4, 8, 16, 20, 32, 40, 64\}$. Where 20 and 40 are the number of physical and virtual cores, respectively.

In \appref{ap:benchmark}, we provide an example of one of the benchmarks we used. We show the description of the problem that this benchmark solves and the implementation of each variant we generated to use in this study. It can also be accessed on GitHub at \url{http://github.com/green-haskell/concurrency-benchmark}. In this repository, we made publicly available the implementation of all benchmarks variants as well as the scripts and instructions necessary to replicate this experiment.


\section{Study Results}\label{sec:results}
In this section, we report the results of our experiments with concurrent Haskell programs. The results are presented in Figures \ref{fig:conc_bench1}, \ref{fig:conc_bench2} and \ref{fig:conc_bench3}. Here, the odd rows are energy consumption results, while the even rows are the corresponding execution time results. We omitted the experiments using 64 capabilities in order to make the charts more readable. The charts including this configuration as well as all the data and source code used in this study are available at \href{http://green-haskell.github.io/}{green-haskell.github.io}.
\newline

\begin{figure*}[tp]
\caption{Energy and Time results for Benchmarks 1, 2, 3 and 7}
\centering
$
\begin{array}{ccc}
 \includegraphics[width=.48\textwidth]{images/conc_bench/chameneos-redux-energy} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/dining-philosophers-energy} \\
 \\

 \includegraphics[width=.48\textwidth]{images/conc_bench/chameneos-redux-time} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/dining-philosophers-time} \\

 \includegraphics[width=.48\textwidth]{images/conc_bench/fasta-energy} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/k-nucleotide-energy}  \\

 \includegraphics[width=.48\textwidth]{images/conc_bench/fasta-time} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/k-nucleotide-time}  \\
\end{array}
$
\footnotesize{Source: Made by the author}
\label{fig:conc_bench1}
\end{figure*}

\begin{figure*}[tp]
\caption{Energy and Time results for Benchmarks 4, 5, 6 and 8}
\centering
$
\begin{array}{ccc}
 \includegraphics[width=.48\textwidth]{images/conc_bench/mandelbrot-energy} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/regex-dna-energy} \\

 \includegraphics[width=.48\textwidth]{images/conc_bench/mandelbrot-time} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/regex-dna-time} \\
 \\

 \includegraphics[width=.48\textwidth]{images/conc_bench/spectral-norm-energy} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/tsearch-energy} \\

 \includegraphics[width=.48\textwidth]{images/conc_bench/spectral-norm-time} &
 \includegraphics[width=.48\textwidth]{images/conc_bench/tsearch-time} \\
\end{array}
$
\footnotesize{Source: Made by the author}
\label{fig:conc_bench2}
\end{figure*}

\begin{figure*}[tp]
\caption{Energy and Time results for Benchmark 9}
\centering
$
\begin{array}{c}
 \includegraphics[width=.48\textwidth]{images/conc_bench/warp-energy} \\
 \includegraphics[width=.48\textwidth]{images/conc_bench/warp-time} \\
\end{array}
$
\\
\footnotesize{Source: Made by the author}
\label{fig:conc_bench3}
\end{figure*}

\state{Small changes can produce big savings.} One of the main findings of this study is that simple refactorings such as switching between thread management constructs can have considerable impact on energy usage. For example, in \spectral, using \forkOn instead of \forkOS with \TVar can save between 25 and 57\% energy, for a number of capabilities ranging between 2 and 40. Although the savings vary depending on the number of capabilities, for \spectral, \forkOn exhibits lower energy usage independently of this number. For \mandelbrot, variants using \forkOS and \forkOn with \MVar exhibited consistently lower energy consumption than ones using \forkIO, independently of the number of capabilities. For the \forkOS variants, the savings ranged from 5.7 to 15.4\% whereas for \forkOn variants the savings ranged from 11.2 to 19.6\%.

This finding also applies to data sharing primitives. In \chameneos, switching from \TMVar to \MVar with \forkOn can yield energy savings of up to 61.2\%. Moreover, it is advantageous to use \MVar independently of the number of capabilities. In a similar vein, in \fasta, going from \TVar to \MVar with \forkIO can produce savings of up to 65.2\%. We further discuss the implications of this finding in \secref{sec:discussion}.
\newline

\state{Faster is not always greener.} Overall, the shapes of the curves in Figures \ref{fig:conc_bench1}, \ref{fig:conc_bench2} and \ref{fig:conc_bench3} are similar. Although, for six of our nine benchmarks, in at least two variants of each one, there are moments where faster execution time leads to a higher energy consumption. For instance, in the \forkOn-\TMVar variant of \regex, the benchmark is 12\% faster when varying the number of capabilities from 4 to 20 capabilities. But at the same time, its energy consumption increases by 51\%. Also, changing the number of capabilities from 8 to 16 in the \forkIO variant of \tsearch makes it 8\% faster and 22\% less energy-efficient.

In one particular benchmark, \fasta, we had strongly divergent results in terms of performance and energy consumption for some of the variants. For this benchmark, the variants  employing \TVar outperformed the ones using \TMVar and \MVar. For example, when using a number of capabilities equal to the number of physical cores of the underlying machine (20), the \forkOS-\TVar variant was 43.7\% faster than the \forkOS-\MVar one. At the same time, the \TVar variants exhibited the worst energy consumption. In the aforementioned configuration, the \forkOS-\TVar variant consumed 87.4\% more energy. We analyze this benchmark in details in \secref{sec:fasta}.
\newline

\state{There is no overall winner.} Overall, no thread management construct or data sharing primitive, or combination of both is the best. For example, the \forkIO-\TMVar variant is one of most energy-efficient for \dining. The \forkOS-\TMVar variant consumes more than six times more energy. However, for the \chameneos benchmark, the \forkIO-\TMVar variant consumes 2.4 times more energy than the best variant, \forkIO-\MVar. This example is particularly interesting because these two benchmarks have similar characteristics. Both \dining and \chameneos are synchronization-intensive benchmarks and both have a fixed number of worker threads. Even in a scenario like this, using the same constructs can lead to discrepant results.
\newline

\state{Choosing more capabilities than available CPUs is harmful.} The performance of most benchmarks is severely impaired by using more capabilities than the number of available CPUs. In \chameneos, for example, moving from 40 to 64 capabilities can cause a 13x slowdown. This suggests that the Haskell runtime system was not designed to handle cases where capabilities outnumber CPU cores.
We discuss this matter further in \chapref{chp:guide}.
%In fact, this assumption makes sense as the official GHC documentation recommends the number of capabilities to match the number of CPU cores. However, the documentation leaves as an open question if virtual cores should be counted. In our experiments, the performance almost never improves after 20 capabilities. So developers should be careful when using more capabilities than available physical CPU cores as it can degrade performance.

%% XXX: when using forkOS, when there is a lot of capabilities, the energy consumption gets worse more rapidly than the performance

\section{Case study: \fasta}\label{sec:fasta}
This section expands the analysis of the \fasta benchmark as it behaves differently from the other benchmarks.

\subsection{How it works}\label{subsec:how}
In \fasta, each worker thread executes independently, generating a piece of the resulting DNA sequence. This piece is generated using a set of random numbers that are calculated by each worker based on a seed value. The current seed is kept in a shared variable. Each worker takes it, calculates the random numbers, and puts a new seed back. The following steps can describe the workers' loop:
\begin{enumerate}
	\item Take \textit{seed0} from the shared variable
	\item Generate \textit{random\_numbers} and \textit{seed1}
	\item Put \textit{seed1} on the shared variable
	\item Compute the DNA sequence based on \textit{random\_numbers}
	\item Wait until the predecessor DNA sequence is written to output
	\item Write DNA sequence to output
\end{enumerate}

As mentioned above, the worker thread has to wait (if needed) to write its DNA sequence after the one generated using the predecessor seed. This step is necessary to guarantee that the final DNA sequence is assembled in the correct order. However, it takes approximately the same time for each worker to generate its piece of DNA sequence as the sequences have the same size. So this particular blocking behavior is not a bottleneck as it takes more time to compute the DNA sequence than to write it to output.

\subsection{The fastests consume more energy}
In \figref{fig:conc_bench2}, looking at the results for the \TVar variants of \fasta, we can see they have a peculiar behavior. First, execution time and energy consumption are pretty similar for all three variants and for each capabilities settings. So every thread management strategy impacts performance in the same way for the \TVar variants. This behavior is unusual in other benchmarks where the combination of both thread management strategy and synchronization primitive seems to affect performance differently. Second, the \TVar variants have the best overall execution time while they are the least energy-efficient. This is also an unexpected behavior. In the other benchmarks, when comparing the charts for execution time and energy consumption for the same benchmark, the ordering of the curves is preserved for most cases.

In order to better understand this scenario, we have used the \texttt{eventlog}\footnote{\url{http://ghc.haskell.org/trac/ghc/wiki/EventLog}} feature of \ac{ghc} to profile the execution of \fasta. The log records the activity of the Haskell runtime system throughout the whole program execution. The ThreadScope~\cite{jones:2009} readings of the log can be seen in Figures \ref{fig:threadscope1a} and \ref{fig:threadscope1b} for the \forkIO-\MVar and \forkIO-\TVar variants, respectively. The first row of the report shows the overall CPU activity while the others show the activity on each capability (or \ac{hec}, as named in the pictures). In this particular example, we executed both variants using 20 capabilities. Although only the first two capabilities are shown in these images, the activity of the other capabilities behaves similarly.

\begin{figure}[h]
\caption{ThreadScope readings for the \forkIO-\MVar variant of \fasta}
\centering
\includegraphics[width=\linewidth]{images/fasta-forkIO-MVar-20}
\footnotesize{Source: Made by the author}
\label{fig:threadscope1a}
\end{figure}

\begin{figure}[h]
\caption{ThreadScope readings for the \forkIO-\TVar variant of \fasta}
\centering
\includegraphics[width=\linewidth]{images/fasta-forkIO-TVar-20}
\footnotesize{Source: Made by the author}
\label{fig:threadscope1b}
\end{figure}

As we can see, the overall activity of the \TVar variant is high during the whole execution while the \MVar variant is the opposite. During profiling, the \TVar variant is around 30\% faster than the \MVar variant, but it uses around 8x more CPU resources. These results show that, although the \MVar variant has several worker threads, its execution is mostly sequential. This happens because, as we use an \MVar to store the seed value, only one thread is generating its random numbers at a time. For the \TVar variant, however, we use a \TVar to store the seed value and the whole random number generation is enclosed by a transaction. In this case, the other threads are not blocked as reads to {\TVar}s are non-blocking. This leads to several threads using the same seed to generate random numbers. However, only one thread succeeds in generating these numbers because when the first one that finishes writes the new seed (\texttt{seed1} from the third step of the worker's loop, \secref{subsec:how}) into the shared variable, the other transactions are aborted and retry.

Considering this scenario we just described, a possible explaination for the high energy consumption of the \TVar variants is the transaction that encloses the random number generation being frequently aborted and re-executed. To assess this hyposesis, we have used the \texttt{stm-stats}\footnote{\url{http://hackage.haskell.org/package/stm-stats}} library. This library provides a wrapper to Haskell's \atomically function that tracks the state of each transaction and counts how often the transaction was retried until it succeeded. This function is called \texttt{trackNamedSTM} and besides the \STM action, it also receives as a parameter a \texttt{String} that we can use to identify each transaction. \figref{fig:fasta-stm-stats} shows the output of the \forkIO-\TVar variant of \fasta using the \texttt{trackNamedSTM} function instead of \atomically. As we can see, the assumption is correct. Line 3 shows that, for the transaction that encloses the random number generation, there were 299 transactions that succeeded while 4138 others failed, which represents 13.84x more executions than necessary.

\begin{figure}[htp]
  \centering
  \caption{Output of \texttt{stm-stats} for \forkIO-\TVar variant of \fasta}
	\begin{minted}{text}
STM transaction statistics (2016-07-20 19:16:02.445387 UTC):
Transaction        Commits    Retries      Ratio
generate-numbers       299       4138      13.84
output-sync            261         33       0.13
wait-semaphore           2          2       1.00
  \end{minted}
  \footnotesize{Source: Generated by stm-stats}
  \label{fig:fasta-stm-stats}
\end{figure}

\subsection{The slowest consumes less energy}
Another curious result is the \forkOS-\MVar behavior. Its execution time is considerably worse than all other variants while the energy consumption is in the middle ground. Some profiling has shown that there was no activity in some capabilities. As we can see in Figure \ref{fig:threadscope2}, capabilities 0, 1, and 2 are working normally whereas capabilities 3 and 4 are idle. In this example, we executed the benchmark using 20 capabilities. The others that does not appear in this image are also iddle. This fact matches the results since having less active capabilities implies less parallelization, which makes the program slower. However, this behavior does not seem correct. For some reason, the runtime system is scheduling the threads only for some capabilities. We also noticed that this happens only with the \forkOS-\MVar variant, changing either the thread management construct or the type of shared variable is enough for the scheduler to work as expected. We were also able to reproduce it on other machines. We observed that usually only four capabilities are used while the others stay without work.

\begin{figure}
\caption{ThreadScope readings for the \forkOS-\MVar variant of \fasta}
\centering
\includegraphics[width=\linewidth]{images/fasta-forkOS-MVar-20}
\footnotesize{Source: Made by the author}
\label{fig:threadscope2}
\end{figure}

We believe that this behaviour occurs due to a bug in Haskell's runtime system scheduler and we filed a ticket on the \ac{ghc} bugtracker\footnote{\url{https://ghc.haskell.org/trac/ghc/ticket/12419}}. Simon Marlow, the mantainer and core contributor of the \acl{rts}, confirmed that he found a bug investigating this case. The bug occurs when, for instance, there are two threads on the run queue of a capability and one of them is bound to the current OS thread. In this case, the scheduler would fail to migrate any threads. He submitted a patch to fix this problem\footnote{\url{https://phabricator.haskell.org/D2430}}, which is currently under review. This patch improves parallelism in programs that have lots of bound threads. However, he also points out that after fixing this bug, the ThreadScope readings using \forkIO and \forkOS still do not look identical: \emph{"even after the patch the threadscope profiles don't look identical. I don't think there is an actual problem, just that the program itself isn't very parallel - if you zoom in, there's lots of time in each thread where no work is being done. The difference in scheduling is due to the way that forkOS has to hand-shake with the new thread to get its ThreadId"}.


\section{Discussion}\label{sec:discussion}
Generally, especially in the sequential benchmarks, high performance is a proxy for low energy consumption. In cooperation with our colleagues from University of Beira Interior, we conducted a complementary study~\cite{lima:2016} that highlighted this for a number of different data structure implementations and operations. Concurrency, however, makes the relationship between performance and energy less obvious. Also, there are clear benefits in employing  different thread management constructs and data-sharing primitives. This section examines this in more detail.

Switching between thread management construct is very simple in Haskell. Functions \forkOn, \forkIO, and \forkOS take a computation of type \IO as parameter and produce results of the same type. Thus, the only difficulty is in determining on which capability a thread created via \forkOn will run. This is good news for developers and maintainers. Considering the seven benchmarks where we implemented variants using different data sharing primitives, in five of them the thread management construct had a stronger impact on energy usage than the data sharing primitives. Furthermore, in these five benchmarks and also in \warp it is clearly beneficial to switch between thread management constructs.

Alternating between data sharing primitives is not as easy, but still not hard, depending on the characteristics of the program to be refactored. Going from \MVar to \TMVar and back is straightforward because they have very similar semantics. The only complication is that, since functions operating on \TMVar produce results of type \STM, calls to these functions must be enclosed in calls to \atomically to produce a result of type \IO. Going from \MVar to \TVar and back is harder, though. If a program using \MVar does not require condition-based synchronization, it is possible to automate this transformation in a non-application-dependent manner~\cite{soares-neto:2014}. If condition-based synchronization is necessary, such as is the case with the \dining benchmark, the semantic differences between \TVar and \MVar make it necessary for the maintainer to understand details of how the application was constructed.

In spite of the absence of an overall winning thread management construct or data-sharing primitive, we can identify a few cases where \emph{a specific approach excels under specific conditions}. For instance, we can see that in both \mandelbrot and \spectral, \forkOn has a slightly better performance than \forkIO and \forkOS. In \mandelbrot, the \forkOn variants are around 20\% more energy-efficient than the \forkIO variants. In \spectral, \forkOn can be up to 2x greener than \forkOS. These two benchmarks are both CPU-intensive. They also create as many threads as the number of capabilities. In a scenario such as this, a computation-heavy algorithm with few synchronization points, keeping each thread executing in a dedicated CPU core is beneficial for the performance. This is precisely what \forkOn does. We further explore this topic in \chapref{chp:guide}.

Although there is no overall winner, for most benchmark we can point out a configuration that beats the others in terms of energy consumption and performance. We can observe that the ordering of the curves are more or less preserved when comparing the graph of each metric. This is amazing news for developers because: (1) small changes can make big differences; (2) it is very easy to change concurrent constructs; (3) it is cheap to experiment and perform benchmarks; and (4) it is easy to identify which configuration excels.


\section{Threats to Validity}\label{sec:threats}
This work focused on the Haskell programming language. It is possible that its results do not apply to other functional programming languages, especially considering that Haskell is one of the few lazy programming languages in existence. Moreover, we analyzed only a subset of Haskell's constructs for concurrent and parallel programming. It is not possible to extrapolate the results to other constructs for concurrent and parallel execution. Nonetheless, our evaluation comprised a large number of experimental configurations that cover widely-used constructs of the Haskell language.

It is not possible to generalize the results of this study to other hardware platforms for which Haskell programs can be compiled. Factors such as operating system scheduling policies~\cite{yuan:2003} and processor and interconnect layouts~\cite{solernou:2013} can clearly impact the results. We take a route common in experimental programming language research, by constructing experiments over representative system software and hardware, and the results are empirical by nature. To take a step further, we have re-executed the experiments in additional hardware configurations. The primary goal is to understand the stability and portability of our results. We ran some of the benchmarks on another machine, a 4-core Intel i7-3770 (IvyBridge) with 8 GB of DDR 1600 runing Ubuntu Server 14.04.3 LTS (kernel 3.19.0-25) and GHC 7.10.2. \figref{fig:other-machine-conc} shows the results of \mandelbrot running on this i7 machine. The results show analogous trends in which the curves have similar shapes to the results of \figref{fig:conc_bench2}. The same trend can be observed for the remaining benchmarks.

\begin{figure}[t]
\caption{Energy and Time results on Alternative Platform}
\centering
$
\begin{array}{cc}
\includegraphics[width=0.47\linewidth]{images/conc_bench/mandelbrot-i7-energy} &
\includegraphics[width=0.47\linewidth]{images/conc_bench/mandelbrot-i7-time} \end{array}
$
\footnotesize{Source: Made by the author}
\label{fig:other-machine-conc}
\end{figure}

It is also not possible to generalize the results to other versions of GHC. Changes in the runtime system, for example, can lead to different results. This work also did not explore the influence of the various compiler and runtime settings of GHC. As the options range from GC algorithms to scheduling behaviour, it can have a significant impact on performance, especially for concurrency. For the benchmarks we developed, we used the default settings of GHC. For the ones from CLBG, we used the same settings used there to preserve the performance characteristics intended by the developers.

One further threat is related to our measurement approach We have employed RAPL to measure energy consumption. Thus, the results could be different for external measurement equipment. Nonetheless, previous work~\cite{hahnel:2012} has compared the accuracy of RAPL with that of an external energy monitor and the results are consistent.
